{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "private_outputs": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ameer-alwadiya/binary-claasification-ml/blob/main/ACS61011_Project_Starter_2024.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ACS61011 Project Starter Python Code\n",
        "\n",
        "**You should first go to File and 'Save a copy in Drive' for this file**\n",
        "\n",
        "This code is the start code for the ACS61011 project on automated speech recognition.\n",
        "\n",
        "The code does the following:\n",
        "\n",
        "*   Pulls the data in from github\n",
        "*   Unzips the data\n",
        "*   Creates Keras training and validation datasets\n",
        "*   Extracts input-output data from the Keras datasets\n"
      ],
      "metadata": {
        "id": "usc2kSY7fXMU"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w_Nm-NM9c9Vq"
      },
      "outputs": [],
      "source": [
        "# NOTE: YOU SHOULD ONLY NEED TO RUN THIS STEP THE FIRST TIME IN A SESSION\n",
        "import tensorflow as tf\n",
        "\n",
        "# get the data from github and unzip\n",
        "# The term \"wget\" stands for \"World Wide Web get.\" It's a command-line utility for downloading files from the internet.\n",
        "!wget https://raw.githubusercontent.com/andrsn/data/main/speechImageData.zip\n",
        "!unzip -q /content/speechImageData.zip\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-process data into training and validation sets, using Keras dataset objects\n",
        "\n",
        "Note that when the data is unzipped it is stored locally to Google Colab in the content folder and the unzipped folder is called\n",
        "\n",
        "'speechImageData - Copy'\n",
        "\n",
        "and it contains:\n",
        "\n",
        "the training data in the folder TrainData and\n",
        "\n",
        "the validation in the folder ValData\n",
        "\n",
        "There are 12 classes of different spoken words and the spectrograms, which form the input image data are of size 98x50 pixels."
      ],
      "metadata": {
        "id": "Lji0v1zkvmCN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "train_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/speechImageData - Copy/TrainData',\n",
        "    labels='inferred',\n",
        "    color_mode=\"grayscale\",\n",
        "    label_mode='categorical',\n",
        "    batch_size=128,\n",
        "    image_size=(98, 50)\n",
        ")\n",
        "\n",
        "val_ds = tf.keras.utils.image_dataset_from_directory(\n",
        "    directory='/content/speechImageData - Copy/ValData',\n",
        "    labels='inferred',\n",
        "    color_mode=\"grayscale\",\n",
        "    label_mode='categorical',\n",
        "    batch_size=128,\n",
        "    image_size=(98, 50)\n",
        ")"
      ],
      "metadata": {
        "id": "FPTSYPMItpgG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Extract input-output data into arrays, which can be more useful"
      ],
      "metadata": {
        "id": "zsUT9haKa3p2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Extract the  training input images and output class labels\n",
        "x_train = []\n",
        "y_train = []\n",
        "\n",
        "\"\"\"The take method is used to create a new dataset containing a specified number of elements from the original dataset.\n",
        "When -1 is provided as the argument, it essentially means \"take all elements\" from the original dataset.\"\"\"\n",
        "for images, labels in train_ds.take(-1):\n",
        "    x_train.append(images.numpy())\n",
        "    y_train.append(labels.numpy())\n",
        "\n",
        "# axis=0 means the arrays will be concatenated along the first axis (i.e., rows will be stacked vertically).\n",
        "x_train = np.concatenate(x_train, axis=0)\n",
        "y_train = np.concatenate(y_train, axis=0)\n",
        "\n",
        "# Extract the validation input images and output class labels\n",
        "x_val = []\n",
        "y_val = []\n",
        "for images, labels in val_ds.take(-1):\n",
        "    x_val.append(images.numpy())\n",
        "    y_val.append(labels.numpy())\n",
        "\n",
        "x_val = np.concatenate(x_val, axis=0)\n",
        "y_val = np.concatenate(y_val, axis=0)"
      ],
      "metadata": {
        "id": "v22tCYk60i-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Shape of the feature training data: ', x_train.shape)\n",
        "print('Shape of the label training data: ', y_train.shape)\n",
        "print('--------------------------------------------------')\n",
        "print('Shape of an example of the feature training data: ', x_train[0].shape)\n",
        "print('Shape of an example of the label training data: ', y_train[0].shape)\n",
        "print('--------------------------------------------------')\n",
        "print('Shape of the feature testing data: ', x_val.shape)\n",
        "print('Shape of the label testing data: ', y_val.shape)\n",
        "print('--------------------------------------------------')\n",
        "# Get the class names (labels)\n",
        "label_names = train_ds.class_names\n",
        "print('Label names: ', label_names)"
      ],
      "metadata": {
        "id": "wkl_BCJoXMCL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "\n",
        "# Plot some examples\n",
        "num_examples = 5\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "for i in range(num_examples):\n",
        "    plt.subplot(1, num_examples, i + 1)\n",
        "    rand_index = np.random.randint(0, x_train.shape[0])\n",
        "    plt.imshow(x_train[rand_index], plt.cm.binary)\n",
        "    plt.title(f\"Label: {label_names[np.argmax(y_train[rand_index])]}\") # np.argmax() returns the index of the maximum value in the array.\n",
        "    plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "rORzmj9PTWWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Spectrogram Augmentation:"
      ],
      "metadata": {
        "id": "v5dH9TwoY4Kc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Frequency and time masks:**"
      ],
      "metadata": {
        "id": "IgAsbY0XTx6g"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def spec_augment(spec, num_freq_masks=1, num_time_masks=1, freq_masking_max_percentage=0.1, time_masking_max_percentage=0.1):\n",
        "    spec_aug = spec.copy()\n",
        "    max_percentage_freq = int(freq_masking_max_percentage * spec.shape[0])\n",
        "    max_percentage_time = int(time_masking_max_percentage * spec.shape[1])\n",
        "\n",
        "    for _ in range(num_freq_masks):\n",
        "        f = np.random.randint(0, max_percentage_freq)\n",
        "        f0 = np.random.randint(0, spec.shape[0] - f)\n",
        "        spec_aug[f0:f0 + f, :] = 0\n",
        "\n",
        "    for _ in range(num_time_masks):\n",
        "        t = np.random.randint(0, max_percentage_time)\n",
        "        t0 = np.random.randint(0, spec.shape[1] - t)\n",
        "        spec_aug[:, t0:t0 + t] = 0\n",
        "\n",
        "    return spec_aug"
      ],
      "metadata": {
        "id": "se2c7FnpY5ml"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_augmented = np.array([spec_augment(spec) for spec in x_train])\n",
        "\n",
        "print(x_train_augmented.shape)"
      ],
      "metadata": {
        "id": "4dC8VF98ZAXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_combined1 = np.concatenate([x_train, x_train_augmented], axis=0)\n",
        "y_train_combined1 = np.concatenate([y_train, y_train], axis=0)\n",
        "\n",
        "print(x_train_combined1.shape)"
      ],
      "metadata": {
        "id": "sIHYSOh2T5Be"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the original spectrogram\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Spectrogram')\n",
        "plt.imshow(x_train[3], plt.cm.binary)\n",
        "\n",
        "# Display the augmented spectrogram\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Augmented Spectrogram')\n",
        "plt.imshow(x_train_augmented[3], plt.cm.binary)\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "fre7i3zHZDIR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Spectograms mixing:**"
      ],
      "metadata": {
        "id": "wnYgaZ0TUDNj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def mixup(original_melspecs, original_labels, alpha=0.5):\n",
        "    indices = np.random.permutation(original_melspecs.shape[0])\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "\n",
        "    augmented_melspecs = original_melspecs * lam + original_melspecs[indices] * (1 - lam)\n",
        "    augmented_labels = original_labels * lam + original_labels[indices] * (1 - lam)\n",
        "\n",
        "    return augmented_melspecs, augmented_labels"
      ],
      "metadata": {
        "id": "C5Sf5a0eepHg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "augmented_x_train, augmented_y_train = mixup(x_train, y_train)\n",
        "print(augmented_x_train.shape)"
      ],
      "metadata": {
        "id": "vpmIITARep4v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_train_combined2 = np.concatenate([x_train_combined1, augmented_x_train], axis=0)\n",
        "y_train_combined2 = np.concatenate([y_train_combined1, augmented_y_train], axis=0)\n",
        "\n",
        "print(x_train_combined2.shape)\n",
        "print(y_train_combined2.shape)"
      ],
      "metadata": {
        "id": "SnkFgdcjUJdi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display the original spectrogram\n",
        "plt.figure(figsize=(5, 5))\n",
        "plt.subplot(1, 2, 1)\n",
        "plt.title('Original Spectrogram')\n",
        "plt.imshow(x_train[6], plt.cm.binary)\n",
        "\n",
        "\n",
        "# Display the augmented spectrogram\n",
        "plt.subplot(1, 2, 2)\n",
        "plt.title('Augmented Spectrogram')\n",
        "plt.imshow(augmented_x_train[6], plt.cm.binary)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "IOm8XIo6etpo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Main Model:"
      ],
      "metadata": {
        "id": "OHGyVgDMTos2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, Activation, MaxPooling2D, Flatten, Dense, Softmax, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.regularizers import l2  # Import l2 regularizer\n",
        "\n",
        "def create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0.25, learning_rate=0.001, weight_decay=0.001):\n",
        "    model = Sequential()\n",
        "\n",
        "    model.add(Conv2D(filters[0], kernel_size=(3, 3), padding='same', input_shape=(98, 50, 1), kernel_regularizer=l2(weight_decay)))  # Add L2 to kernel\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    for i in range(num_layers-1):\n",
        "        model.add(Conv2D(filters[1], kernel_size=(3, 3), padding='same', kernel_regularizer=l2(weight_decay)))  # Add L2 to all convolutional layers\n",
        "        model.add(BatchNormalization())\n",
        "        model.add(Activation('relu'))\n",
        "        model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding='same'))\n",
        "\n",
        "    model.add(Conv2D(filters[2], kernel_size=(3, 3), padding='same', kernel_regularizer=l2(weight_decay)))  # Add L2 to the last convolutional layer\n",
        "    model.add(BatchNormalization())\n",
        "    model.add(Activation('relu'))\n",
        "    model.add(MaxPooling2D(pool_size =(12, 1), strides=(1, 1), padding = 'same'))\n",
        "    model.add(Dropout(dropout_rate))\n",
        "\n",
        "    model.add(Flatten())\n",
        "\n",
        "    model.add(Dense(12))  # No regularization typically applied to Dense layers\n",
        "    model.add(Softmax())\n",
        "\n",
        "    optimizer = Adam(learning_rate=learning_rate)\n",
        "    model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "0ZGtVUuRT1kC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Training using x_train:**"
      ],
      "metadata": {
        "id": "W1HBAIz9UzPh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "veTl7DoRUqr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Training using x_train_combined1:**"
      ],
      "metadata": {
        "id": "M83xEYTFUz0v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "8vk-i5TaUq1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3- Training using x_train_combined2:**"
      ],
      "metadata": {
        "id": "7TZ7FyPcU0WW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=2, filters=(8, 16, 32), dropout_rate=0, weight_decay=0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train_combined2, y_train_combined2, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "ncE7voDRUq_I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "*The accurcy we got from training the model using x_trained_combined2 (mixing spectogram) indicate the mixing spectogram augmentations has made no improvements to the model as x_trained_combined1 (frequency and time masks) has got the same accurcy.*"
      ],
      "metadata": {
        "id": "B4vsmeHsVitM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Grid Search Optimizatiom:"
      ],
      "metadata": {
        "id": "BS1EH6fAT-ET"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install scikeras # need to install scikeras in colab"
      ],
      "metadata": {
        "id": "DlMVHJ8xURpt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from scikeras.wrappers import KerasClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Create KerasClassifier\n",
        "model = KerasClassifier(build_fn=create_model, epochs=5,\n",
        "                        verbose=1, num_layers=2,\n",
        "                        filters=(8, 16, 32))\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'num_layers': [2, 3, 4, 6, 8],\n",
        "    'filters': [(8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)],\n",
        "}\n",
        "\n",
        "# Perform grid search\n",
        "grid = GridSearchCV(estimator=model, param_grid=param_grid, cv=3)\n",
        "grid_result = grid.fit(x_train, y_train, batch_size=32, epochs=15, validation_split=0.3)\n",
        "\n",
        "# Print results\n",
        "print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n",
        "means = grid_result.cv_results_['mean_test_score']\n",
        "stds = grid_result.cv_results_['std_test_score']\n",
        "params = grid_result.cv_results_['params']\n",
        "for mean, stdev, param in zip(means, stds, params):\n",
        "    print(\"%f (%f) with: %r\" % (mean, stdev, param))"
      ],
      "metadata": {
        "id": "OE5z1hrXirgd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Training using x_train with num_layers=4 and filters=(32, 64, 128):**"
      ],
      "metadata": {
        "id": "nCKIF3lRJJZ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0, weight_decay=0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "W44mnVozKPGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Training using x_train_combined1 with num_layers=4 and filters=(32, 64, 128):**"
      ],
      "metadata": {
        "id": "jJqjxhDZJKD_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0, weight_decay=0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "2RPwsxQsGm4w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Training using x_train_combined2 with num_layers=4 and filters=(32, 64, 128):**"
      ],
      "metadata": {
        "id": "3nhdsIM0JKxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=4, filters=(32, 64, 128), dropout_rate=0, weight_decay=0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train_combined2, y_train_combined2, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "0eq1mGxbKSGC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bayesian Optimization:\n"
      ],
      "metadata": {
        "id": "GwWitmV6X_6c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install hyperopt"
      ],
      "metadata": {
        "id": "LF8JTsoKYAss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from hyperopt import fmin, tpe, hp\n",
        "# fmin: Stands for \"Function Minimization\".\n",
        "# tpe: Stands for \"Tree-structured Parzen Estimator\".\n",
        "# hp: Stands for \"Hyperparameters\".\n",
        "\n",
        "# Define the search space\n",
        "space = {\n",
        "    'num_layers': hp.choice('num_layers', [2, 3, 4, 6, 8]),\n",
        "    'filters': hp.choice('filters', [(8, 16, 32), (16, 32, 64), (32, 64, 128), (64, 128, 256), (128, 256, 512)]),\n",
        "\n",
        "    'dropout_rate': hp.choice('dropout_rate', [0.0, 0.1, 0.15, 0.2, 0.25]),\n",
        "    'weight_decay': hp.choice('weight_decay', [0.0, 0.1, 0.01, 0.001, 0.0001]),\n",
        "}\n",
        "\n",
        "# Define the objective function\n",
        "def objective(params):\n",
        "    model = create_model(**params)  # Create model with current parameters\n",
        "\n",
        "    # Train the model and evaluate on the test set\n",
        "    model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
        "    loss, accuracy = model.evaluate(x_val, y_val)\n",
        "    return {'loss': -accuracy, 'status': 'ok'}\n",
        "\n",
        "# Run the optimization\n",
        "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=20)\n",
        "\n",
        "print(\"Best parameters found:\")\n",
        "print(best)"
      ],
      "metadata": {
        "id": "VipTAvOxYCfg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**1- Training using x_train with dropout_rate=0.25, weight_decay=0.0, layers=4, and filters=(64, 128, 256):**\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qIwMoT2UYqsC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=4, filters=(64, 128, 256), dropout_rate=0.25, weight_decay=0.0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train, y_train, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "tJk27sRIYqUu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**2- Training using x_train_combined1 with dropout_rate=0.25, weight_decay=0.0, layers=4, and filters=(64, 128, 256):**"
      ],
      "metadata": {
        "id": "INRPiDl3YyU_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=4, filters=(64, 128, 256), dropout_rate=0.25, weight_decay=0.0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "CwUioTHsNkG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**3- Training using x_train_combined2 with dropout_rate=0.25, weight_decay=0.0, layers=4, and filters=(64, 128, 256):**"
      ],
      "metadata": {
        "id": "baIWh18BN0iZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "optimal_model = create_model(num_layers=4, filters=(64, 128, 256), dropout_rate=0.25, weight_decay=0.0)\n",
        "\n",
        "# Train the model\n",
        "optimal_model.fit(x_train_combined2, y_train_combined2, batch_size=64, epochs=15, validation_split=0.5)\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "9eqh1khVNzwe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Monitoring Training Progress:"
      ],
      "metadata": {
        "id": "fMyIM4xMSzLn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model, compile it with appropriate optimizer and loss\n",
        "\n",
        "# Train the model\n",
        "history = optimal_model.fit(x_train_combined1, y_train_combined1, batch_size=16, epochs=20, validation_split=0.3)\n",
        "\n",
        "# Access training history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "train_accuracy = history.history['accuracy']\n",
        "val_accuracy = history.history['val_accuracy']\n",
        "\n",
        "# Plot training history\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "epochs = range(1, len(train_loss) + 1)\n",
        "\n",
        "plt.plot(epochs, train_loss, 'b', label='Training loss')\n",
        "plt.plot(epochs, val_loss, 'r', label='Validation loss')\n",
        "plt.title('Training and validation loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "plt.plot(epochs, train_accuracy, 'b', label='Training accuracy')\n",
        "plt.plot(epochs, val_accuracy, 'r', label='Validation accuracy')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "7sWkAwWPTHLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Accuracy:\", test_accuracy)"
      ],
      "metadata": {
        "id": "sK2fZnEwUJ1u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.callbacks import EarlyStopping\n",
        "\n",
        "# Define the early stopping callback\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "\n",
        "# Train the model with early stopping\n",
        "history = optimal_model.fit(x_train_combined1, y_train_combined1, batch_size=64, epochs=15,\n",
        "                             validation_split=0.5, callbacks=[early_stopping])\n",
        "\n",
        "# Evaluate the model on test data\n",
        "test_loss, test_accuracy = optimal_model.evaluate(x_val, y_val)\n",
        "print(\"Test Loss:\", test_loss)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n"
      ],
      "metadata": {
        "id": "nqRBIBXRUzpP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# Define the grid search parameters\n",
        "param_grid = {\n",
        "    'batch_size': [16, 32, 64, 128, 256],\n",
        "    'epochs': [20, 30, 40, 50, 60],\n",
        "    'validation_split': [0.1, 0.2, 0.3, 0.4, 0.5],\n",
        "}\n",
        "\n",
        "# Generate combinations of hyperparameters\n",
        "grid = ParameterGrid(param_grid)\n",
        "\n",
        "best_params = None\n",
        "best_score = float('inf')\n",
        "\n",
        "# Iterate over each combination of hyperparameters\n",
        "for params in grid:\n",
        "    print(\"Training with params: \", params)\n",
        "\n",
        "    # Create model\n",
        "    model = create_model(num_layers=4,\n",
        "                         filters=(64, 128, 256),\n",
        "                         dropout_rate=0.25,\n",
        "                         weight_decay=0.0)\n",
        "\n",
        "    # Fit model\n",
        "    history = model.fit(x_train_combined1,\n",
        "                        y_train_combined1,\n",
        "                        batch_size=params['batch_size'],\n",
        "                        epochs=params['epochs'],\n",
        "                        validation_split=params['validation_split'],\n",
        "                        )\n",
        "\n",
        "    # Evaluate model\n",
        "    val_loss = model.evaluate(x_val, y_val, verbose=0)\n",
        "\n",
        "    print(\"Validation Loss:\", val_loss)\n",
        "\n",
        "    # Check if this is the best model so far\n",
        "    if val_loss[0] < best_score:\n",
        "        best_score = val_loss\n",
        "        best_params = params\n",
        "\n",
        "print(\"Best parameters found:\", best_params)"
      ],
      "metadata": {
        "id": "bYiePSAFZIg9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}